# Optimizer , Activation funtion and Loss Function 

## Table of Contents

- [About](#about)
- [Getting Started](#getting_started)
- [Implementation](#implementation-a-name--implementationa)
- [Usage](#usage)
- [Contributing](../CONTRIBUTING.md)

## About <a name = "about"></a>
This repro is basic implementation of deep learning optimizer , Activation function and loss Function
Inspired by :
[SUNNY BHAVEEN CHANDRAN](https://github.com/c17hawke)

## Getting Started <a name = "getting_started"></a>
 
create env with python 3.8 version

```bash
conda create --prefix ./env python=3.8 -y
```
### Prerequisites


```
pip install -r requirements.txt
```

### Implementation <a name = "implementation"></a>

[Activation function](https://nbviewer.org/github/pk1308/Activations-Loss---Fast-Optimizers/blob/main/Activation%20function/activation%20functiom.ipynb)
[Batch_normalization](https://nbviewer.org/github/pk1308/Activations-Loss---Fast-Optimizers/blob/455aa23957119ecda2ccfc1e6096f804934c7b96/Batch_noramlization/Batch_normalisation.ipynb)
[momentum optimizer](https://nbviewer.org/github/pk1308/Activations-Loss---Fast-Optimizers/blob/9c6389ec79ff96f96ea34a5462275617052ad57c/optimizer/momentum_optimizer.ipynb)
[NAG](https://nbviewer.org/github//pk1308/Activations-Loss---Fast-Optimizers/blob/59813470c066193ac3c7e94698630aa1fb21038d/optimizer/NAG.ipynb)
[ADAGRAD](https://nbviewer.org/github/pk1308/Activations-Loss---Fast-Optimizers/blob/main/optimizer/Adagrad.ipynb)


## Usage <a name = "usage"></a>

Add notes about how to use the system.
